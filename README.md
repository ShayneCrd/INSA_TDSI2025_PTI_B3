
# MS Brain Lesion Segmentation Benchmark (PTI Project)

## Introduction

This PTI project focuses on **3D brain lesion segmentation in Multiple Sclerosis (MS)**.
Several deep learning methods have been investigated, all showing different strengths depending on the evaluation metrics.

This project is built around **four specific datasets** that must be used for the pipeline to work as intended.  
However, it is also possible to experiment with **custom datasets**, provided they are available in **`.nii.gz` format** and converted to the **nnU-Net format**.

---

## Pre-requisites and Methods

This project compares three segmentation approaches:

- **LST-AI**  
  A state-of-the-art MS lesion segmentation tool using three pre-trained U-Nets.  
  Only **one-shot inference** is performed.

- **SegResNet (MONAI)**  
  A residual segmentation network implemented in the MONAI framework.

- **nnU-Net v2**  
  A self-configuring U-Net framework used both as a standalone segmentation method and as a **reference for cross-validation folds**.

---

## Important Notice

### Environment
For users with **ENS-Lyon accounts**, it is **mandatory** to use the `monai-dev` conda environment, which contains all required dependencies.

If you are **not on an ENS system**, follow this installation guide:  
https://www.creatis.insa-lyon.fr/~grenier/?p=409

### Pipeline dependency
This project strictly follows **nnU-Net conventions**:

- All datasets are converted to **nnU-Net format**
- **nnU-Net must be run first**
- **SegResNet cross-validation relies on folds generated by nnU-Net**

Running SegResNet **before nnU-Net** will lead to errors.

### LST-AI
LST-AI inference was performed on a **separate machine**, as it requires:
- HD-BET
- greedy registration tool
- a dedicated virtual environment

Official setup instructions are available here:  
https://github.com/CompImg/LST-AI

Scripts are provided to:
- run LST-AI inference
- convert predictions to **nnU-Net-compatible format**
- store them in `lstFrame/`

### Hardware
All experiments were conducted on an **RTX 3090 Ti (24 GB VRAM)**.

Notes:
- RTX 4000 series may work
- RTX 5000 series (e.g. RTX 5090) caused **compatibility issues** with nnU-Net and SegResNet

---

## Step-by-step Execution Guide

first, start by cloning the repository on your machine:

```bash

git clone https://github.com/ShayneCrd/INSA_TDSI2025_PTI_B3.git
```
then deactivate the current conda environment for safety and activate monai-dev or you current working environment
```bash
conda deactivate
conda activate monai-dev  #or the name of your working environment
```
We then set the environment variables for nnUNet_raw, nnUNet_preprocessed, nnUNet_results. This is done to automate nnUNet:

```bash
vim ~/.bashrc
```
press on esc then i for insert mode
at the bottom, write:

```bash
export nnUNet_raw="/path/to/nnUnetFrame/nnUNet_raw"
export nnUNet_preprocessed="/path/to/nnUnetFrame/nnUNet_preprocessed"
export nnUNet_results="/path/to/nnUnetFrame/nnUNet_results"
```
press on esc then write :wq 

 
Move your un-formatted datasets into the nnUnetFrame directory. In our case: MSLesSeg, MICCAI2016_test, MICCAI2016_train, Open_ms_data-master

We will then convert each dataset into the nnUnet format and store them in the nnUNet_raw folder. In our project, we defined the new formatted names:

MSLesSeg = Dataset421_TDSI2025
MICCAI2016 = Dataset422_TDSI2025
Open_ms (longitudinal) = Dataset423_TDSI2025
Open_ms (cross-sectional) = Dataset424_TDSI2026

in your terminal:

cd path/to/nnUnetFrame

**Format Dataset 421:**
```bash
convertMsLesSeg_to_nnUnet.py
```

**Format Dataset 422:**

```bash
python convert_MICCAI_to_nnUNet.py
```

if it doesn't work , check the paths at the top of the script: 
MICCAI_test_path, 
MICCAI_train_path, 
imagesTr_path
labelsTr_path 
imagesTs_path 
dataset_json_path 

Once the dataset422_TDSI has been created in nnUnet_raw, we have to fix the affines on the labels (an issue noticed using SegResNet). We basically resample all modalities based on one chosen modality. ref_mode 2 => each image is resampled in the flair space. --inplace overwrites the dataset_raw mentionned

```bash
python affine_fix422.py \
    --dataset_raw path/to/nnUNet_raw/Dataset422_TDSI2025 \
    --ref_mod 2 \ 
    --fix_labels
    --inplace
```

**Format Dataset 423:**
```bash
python Convert_OpenMS_to_nnUnet.py
```

**Format Dataset 424:**
```bash
python convert_openMS2_to_nnUnet.py
python fix_424_mismatch.py
```
then after 423 and 424 dataset have been formatted as detailed:
```bash
skullstrip_and_diff_423_424.py
```
The datasets conversion and pre-processing is done, we can now verify the datasets integrity using nnU-Net.
Anywhere in the project directory, run:

**For Dataset421:**
```bash
nnUNetv2_plan_and_preprocess -d 421 --verify_dataset_integrity"
```
**For Dataset422:**
```bash
nnUNetv2_plan_and_preprocess -d 422 --verify_dataset_integrity"
```
**For Dataset423:** 
```bash
nnUNetv2_plan_and_preprocess -d 423 --verify_dataset_integrity"
```
**For Dataset424:**
```bash
nnUNetv2_plan_and_preprocess -d 424 --verify_dataset_integrity"
```

this step will create a new subfolder in your nnUNet_preprocessed folder named after the dataset. Once the command is completed there will be a dataset_fingerprint.json,a nnUNetPlans as well as subfolders containing the preprocessed data for the UNet configurations. Under nUNet_preprocessed, check which one of these plans is present: nnUNetPlans_2d, nnUNetPlans_3d_lowres, nnUNetPlans_3d_fullres, and if splits_final.json is present (useful for cross validation training
---
###Training on datasets

 We have trained all datasets on 3d_fullres but you can change the UNet_CONFIGURATION in the command:
nnUNetv2_train DATASET_ID UNET_CONFIGURATION FOLD -npz

we are performing cross validation training, fold by fold, once a fold is trained, launch the next fold training, one fold can take up to 1 day to fully train. At each epoch, nnU-Net saves the  best model found (based on pseudo dice) and the latest model. If you want to continue a training, use the extension --c:

**Full training example on dataset 421**
```bash
nnUNetv2_train 421 3d_fullres 4 -npz
nnUNetv2_train 421 3d_fullres 3 -npz
nnUNetv2_train 421 3d_fullres 2 -npz
nnUNetv2_train 421 3d_fullres 1 -npz
nnUNetv2_train 421 3d_fullres 0 -npz
```
For other datasets, just replace the Dataset_id by one in the list {421,422,423,424}

once all the datasets have been trained,we have created an empty folder called **pred_all_folds** under **nnUNet_results/DatasetXXX_TDSI2025/**, next to the **folder nnUNetTrainer__nnUNetPlans__3d_fullres**. pred_all_folds is where we store the nnUNet infered masks.
Those should be the two folders for each dataset in nnUNet_results. 

 we can now perform the inference:

###Inference on datasets

for each dataset, run:

```bash
nnUNetv2_predict -i INPUT_FOLDER -o OUTPUT_FOLDER -d DATASET_NAME_OR_ID -c CONFIGURATION --save_probabilities -chk checkpoint_best.pth
```

example:
```bash
nnUNetv2_predict -i /local/scardell/nnUnetFrame/nnUNet_raw/Dataset421_TDSI2025/ -o /local/scardell/nnUnetFrame/nnUNet_results/Dataset421_TDSI2025/pred_all_folds/ -d 421 -c 3d_fullres --save_probabilities -chk checkpoint_best.pth
```
Once nnUNet inference is performed for each dataset, we can move on to SegResNet

---

###Training on SegResNet:

if you are in the root of the project:

```bash
cd SegResNetFrame
```

**train_SegResNet.py** is the script which performs the training. The cross validation training is implemented with the bash script : **automate_cv_train_2.sh** which basically automates train_segResNet based on the folds determined by nnUNet.

**for each dataset**, run:

```bash
bash automate_cv_train_2.sh Dataset_id 
```
for example: 
```bash
bash automate_cv_train_2.sh 421
```
This scripts saves each fold model in **SegResNetFrame/SegResNet_results/cross_validation_training/** under the datasets name.

###Infering on SegResNet:

After all the datasets have been trained, we can perform the inference.
in the directory **SegResNetFrame**, run:

**adapt the path before nnUnetFrame to yours, here it's /local/scardell**

**Dataset421**
```bash
python infer_SegResNet.py     --nnunet_raw /local/scardell/nnUnetFrame/nnUNet_raw/Dataset421_TDSI2025/     --cv_dir /local/scardell/SegResNetFrame/SegResNet_results/cross_validation_training/Dataset421/     --out /local/scardell/SegResNetFrame/SegResNet_inference/cross_val_inference/     --modalities 0,1,2     --patch 128,128,32   --save_prob --use_cv
```

**Dataset422**
```bash
python infer_SegResNet.py     --nnunet_raw /local/scardell/nnUnetFrame/nnUNet_raw/Dataset422_TDSI2025/     --cv_dir /local/scardell/SegResNetFrame/SegResNet_results/cross_validation_training/Dataset422/     --out /local/scardell/SegResNetFrame/SegResNet_inference/cross_val_inference/     --modalities 0,1,2     --patch 128,128,32   --save_prob --use_cv
```

**Dataset423**
```bash
python infer_SegResNet.py     --nnunet_raw /local/scardell/nnUnetFrame/nnUNet_raw/Dataset423_TDSI2025/     --cv_dir /local/scardell/SegResNetFrame/SegResNet_results/cross_validation_training/Dataset423/     --out /local/scardell/SegResNetFrame/SegResNet_inference/cross_val_inference/     --modalities 0,1,2,3,4,5,6,7,8     --patch 128,128,32     --save_prob --use_cv
```

**Dataset424**
```bash
python infer_SegResNet.py     --nnunet_raw /local/scardell/nnUnetFrame/nnUNet_raw/Dataset424_TDSI2025/     --cv_dir /local/scardell/SegResNetFrame/SegResNet_results/cross_validation_training/Dataset424/     --out /local/scardell/SegResNetFrame/SegResNet_inference/cross_val_inference/     --modalities 0,1,2,3     --patch 128,128,32     --save_prob --use_cv
```

After the inference is stored, the generated masks and probability maps are found under: **.../SegResNetFrame/SegResNet_results/cross_validation_training/Dataset_id/**.  The inference folders contain a mask and its probability map: [case_id].nii.gz, [case_id]_prob.nii.gz. 

###LST-AI:

If you have been able to install and setup lst-AI, we have provided an automation for inference, as it only perform one-shot segmentations on a T1 + FLAIR pair.Go in the project's root directory:

```bash
cd lstFrame
python automate_lst.py [Dataset_id] 
```
where Dataset_id can take the values:  [421,422,423,424]
After that, you should have all inferences stored by dataset id, stored in lstFrame. The inference folders contain a mask and its probability map: [case_id].nii.gz, [case_id]_prob.nii.gz. 

###Benchmarking:

Now that we have our nnUNet, SegResNet and lst-AI predictions, we can calculate the metrics to evaluate their performance. Prior to this action, it is always interesting to visualize the predictions and compare them to the ground truths stored in the labelsTs section in nnUNet_raw. We have used the viewer provided by Remy Cohan: https://www.neuropsis.org/nifti_viewer.html, which is a drag-and-drop solution.


each dataset is benchmarked separately:

**adapt the path roots to yours**
```bash
python benchmark.py --dataset_id 421 --labelsTs /path/to/nnUnetFrame/nnUNet_raw/Dataset421_TDSI2025/labelsTs/ --models '{"nnunet":"/path/to/nnUnetFrame/nnUNet_results/Dataset421_TDSI2025/pred_all_folds/","segresnet":"/path/to/SegResNetFrame/SegResNet_inference/Dataset421_inference/","lst":"/path/to/lstFrame/preds_421/"}' --iou_thresholds '0.2,0.5' --save_json benchmark_results/Dataset421

python benchmark.py --dataset_id 422 --labelsTs /path/to/nnUnetFrame/nnUNet_raw/Dataset422_TDSI2025/labelsTs/ --models '{"nnunet":"/path/to/nnUnetFrame/nnUNet_results/Dataset422_TDSI2025/pred_all_folds/","segresnet":"/path/to/SegResNetFrame/SegResNet_inference/Dataset422_inference/","lst":"/path/to/lstFrame/preds_422"}' --iou_thresholds '0.2,0.5' --save_json benchmark_results/Dataset422
```
...Same reasoning for dataset 423 and 424, just adapt the dataset id each time




